Metadata-Version: 2.1
Name: createllm
Version: 0.1.8
Summary: A Python package that enables users to create and train their own Language Learning Models (LLMs) from scratch using custom datasets. This package provides a simplified approach to building, training, and deploying custom language models tailored to specific domains or use cases.
Home-page: https://github.com/khushaljethava/createllm
Author: Khushal Jethava
Author-email: khushaljethwa14@gmail.com
License: MIT license
Description: # createllm
        
        A Python package that enables users to create and train their own Language Learning Models (LLMs) from scratch using custom datasets. This package provides a simplified approach to building, training, and deploying custom language models tailored to specific domains or use cases.
        
        ## ðŸŽ¯ Core Purpose
        
        createllm allows you to:
        - Train custom language models on your specific text data
        - Create domain-specific LLMs for specialized applications
        - Build and experiment with different model architectures
        - Deploy trained models for text generation tasks
        
        ## âœ¨ Key Features
        
        - ðŸ”¨ Build LLMs from scratch using your own text data
        - ðŸš€ Multi-threaded training for faster model development
        - ðŸ“Š Real-time training progress tracking
        - ðŸŽ›ï¸ Configurable model architecture
        - ðŸ’¾ Easy model saving and loading
        - ðŸŽ¯ Custom text generation capabilities
        - ðŸ“ˆ Built-in performance monitoring
        
        ## ðŸ“‹ Requirements
        
        ```bash
        pip install torch torchvision tqdm dill
        ```
        
        ## ðŸš€ Quick Start Guide
        
        ### 1. Prepare Your Training Data
        
        Place your training text in a file. The model learns from this text to generate similar content.
        
        ```
        my_training_data.txt
        â”œâ”€â”€ Your custom text
        â”œâ”€â”€ Can be articles
        â”œâ”€â”€ Documentation
        â””â”€â”€ Any text content you want the model to learn from
        ```
        
        ### 2. Train Your Custom LLM
        
        ```python
        from createllm import ModelConfig, GPTTrainer, TextFileProcessor
        
        # Initialize model configuration
        config = ModelConfig(
            vocab_size=None,  # Will be automatically set based on your data
            n_embd=384,      # Embedding dimension
            block_size=256,  # Context window size
            n_layer=4,       # Number of transformer layers
            n_head=4        # Number of attention heads
        )
        
        # Create trainer instance
        trainer = GPTTrainer(
            text_file="path/to/my_training_data.txt",
            learning_rate=3e-4,
            batch_size=64,
            max_iters=5000,
            eval_interval=500,
            saved_path="path/to/save/model"
        )
        
        # Start training
        trainer.trainer()  # This will automatically process text and train the model
        ```
        
        ### 3. Use Your Trained Model
        
        ```python
        from createllm import LLMModel
        
        # Load your trained model
        model = LLMModel("path/to/saved/model")
        
        # Generate text
        generated_text = model.generate("Your prompt text")
        print(generated_text)
        ```
        
        ## ðŸ“ Example Use Cases
        
        1. **Domain-Specific Documentation Generator**
        ```python
        # Train on technical documentation
        trainer = GPTTrainer(
            text_file="technical_docs.txt",
            saved_path="tech_docs_model"
        )
        trainer.trainer()
        ```
        
        2. **Custom Writing Style Model**
        ```python
        # Train on specific author's works
        trainer = GPTTrainer(
            text_file="author_works.txt",
            saved_path="author_style_model"
        )
        trainer.trainer()
        ```
        
        3. **Specialized Content Generator**
        ```python
        # Train on specific content type
        trainer = GPTTrainer(
            text_file="specialized_content.txt",
            saved_path="content_model"
        )
        trainer.trainer()
        ```
        
        ## âš™ï¸ Model Configuration Options
        
        Customize your model architecture based on your needs:
        
        ```python
        config = ModelConfig(
            n_embd=384,     # Larger for more complex patterns
            block_size=256, # Larger for longer context
            n_layer=8,      # More layers for deeper understanding
            n_head=8,       # More heads for better pattern recognition
            dropout=0.2     # Adjust for overfitting prevention
        )
        ```
        
        ## ðŸ’¡ Training Tips
        
        1. **Data Quality**
           - Clean your training data
           - Remove irrelevant content
           - Ensure consistent formatting
        
        2. **Resource Management**
           ```python
           trainer = GPTTrainer(
               batch_size=32,     # Reduce if running out of memory
               max_iters=5000,    # Increase for better learning
               eval_interval=500  # Monitor training progress
           )
           ```
        
        3. **Model Size vs Performance**
           - Smaller models (n_layer=4, n_head=4): Faster training, less complex patterns
           - Larger models (n_layer=8+, n_head=8+): Better understanding, more resource intensive
        
        ## ðŸ” Monitoring Training
        
        The training process provides real-time feedback:
        ```
        step 0: train loss 4.1675, val loss 4.1681
        step 500: train loss 2.4721, val loss 2.4759
        step 1000: train loss 1.9842, val loss 1.9873
        step 1500: train loss 1.1422, val loss 1.1422
        ...
        ```
        
        ## ðŸ“ Saved Model Structure
        
        ```
        saved_model/
        â”œâ”€â”€ model.pt           # Model weights
        â”œâ”€â”€ encoder.pickle    # Text encoder
        â”œâ”€â”€ decoder.pickle    # Text decoder
        â””â”€â”€ config.json      # Model configuration
        ```
        
        ## âš ï¸ Limitations
        
        - Training requires significant computational resources
        - Model quality depends on training data quality
        - Larger models require more training time and resources
        
        ## ðŸ¤ Contributing
        
        Contributions are welcome! Please feel free to submit pull requests.
        
        ## ðŸ“« Support
        
        For issues and questions, please open an issue in the GitHub repository.
        
        ## ðŸ“„ License
        
        This project is licensed under the MIT License.
        
        ## ðŸ™ Acknowledgments
        
        Based on the GPT architecture with modifications for custom training and ease of use.
        
Keywords: createllm
Platform: UNKNOWN
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: End Users/Desktop 
Classifier: License :: OSI Approved :: MIT License
Classifier: Natural Language :: English
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Requires-Python: >=3.5
Description-Content-Type: text/markdown
