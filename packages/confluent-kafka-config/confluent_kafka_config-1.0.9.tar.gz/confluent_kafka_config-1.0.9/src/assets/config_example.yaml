# template by which we can instantiate a pool of consumers/producers, schema registry and an Admin Client
# to help us avoid boilerplate coding. Seems to me that confluent-kafka's APIs are a tad too low-level than I 
# need them to be.

# Also, if we want, we can use a file similar to src.config_example to create entire environment and fill
# src.yaml_to_pydantic

kafka:
  admin:
    config:
      bootstrap.servers: localhost:9092

  schema_registry:
    url: http://localhost:8081

  consumers:
    - name: consumer1
      topic:
        name: topic1
        # list[int] | None
        # if None, the consumer will subscribe to the topic
        # otherwise, it will use .assign function for TopicPartitions
        # ex.
        partitions:
          - 0
          - 1
        schema_name: schema1
      config:
        bootstrap.servers: localhost:9092
        group.id: grp1
        auto.offset.reset: earliest
        connections.max.idle.ms: 180000

  producers:
    - name: producer1
      topic:
        name: topic2
        # list[int] | None
        # if None (partitions: ), the producer will produce to the topic w/o partition
        # otherwise, it will use partition numbers to send messages
        partitions:
          - 2
          - 6
        schema_name: schema2
      config:
        bootstrap.servers: hostname:9092
        acks: 1
        linger.ms: 1000
        batch.size: 1000
