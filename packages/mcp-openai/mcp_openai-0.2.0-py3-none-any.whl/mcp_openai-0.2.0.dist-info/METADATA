Metadata-Version: 2.4
Name: mcp-openai
Version: 0.2.0
Summary: MCP Client with OpenAI compatible API
Project-URL: Homepage, https://github.com/S1M0N38/mcp-openai
Project-URL: Issues, https://github.com/S1M0N38/mcp-openai/issues
Project-URL: Repository, https://github.com/S1M0N38/mcp-openai
Project-URL: Changelog, https://github.com/S1M0N38/mcp-openai/blob/main/CHANGELOG.md
Author-email: S1M0N38 <bertolottosimone@gmail.com>
License-File: LICENSE
Classifier: Development Status :: 1 - Planning
Classifier: Framework :: Pytest
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.11
Requires-Dist: mcp>=1.1.2
Requires-Dist: openai>=1.58.1
Requires-Dist: python-dotenv>=1.0.1
Description-Content-Type: text/markdown

<div align="center">
  <h1>ùîê&nbsp;&nbsp;mpc-openai&nbsp;&nbsp;‚úß</h1>
  <p><em>MCP Client with OpenAI compatible API</em></p>
</div>

> Model Context Protocol (MCP) is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications.
>
> ‚Äî [https://modelcontextprotocol.io](https://modelcontextprotocol.io)

______________________________________________________________________

> [!WARNING]
> This project is still in the early stages of development. Support is not planned.

This is a MCP **client** (not a server). It is meant to be used as a library for building LLMs UI that support MCP through an OpenAI compatible API. This opens the door to locally runnable inference engines ([vLLM](https://github.com/vllm-project/vllm), [Ollama](https://github.com/ollama/ollama), [TGI](https://github.com/huggingface/text-generation-inference), [llama.cpp](https://github.com/ggerganov/llama.cpp), [LMStudio](https://github.com/lmstudio-ai), ...) that support providing support for the OpenAI API ([text generation](https://platform.openai.com/docs/guides/text-generation), [function calling](https://platform.openai.com/docs/guides/function-calling), etc.).

## Usage

It is highly recommended to use [uv](https://docs.astral.sh/uv/) in your project based on mpc-openai:

- It manages python installation and virtual environment.
- It is an executable that can run self-contained python scripts (in our case MCP server)
- It is used for CI workflows.

Add mcp-openai to your project dependencies with:

```
uv add mcp-openai
```

or use classic pip install.

### Create a MCP client

Now you can create a MCP client by specifying your custom configuration.

```python
from mcp_openai import MCPClient
from mcp_openai import config

mcp_client_config = config.MCPClientConfig(
    mcpServers={
        "the-name-of-the-server": config.MCPServerConfig(
            command="uv",
            args=["run", "path/to/server/scripts.py/or/github/raw"],
        )
        # add here other servers ...
    }
)

llm_client_config = config.LLMClientConfig(
    api_key="api-key-for-auth",
    base_url="https://api.openai.com/v1",
)

llm_request_config = config.LLMRequestConfig(model=os.environ["MODEL_NAME"])

client = MCPClient(
    mcp_client_config,
    llm_client_config,
    llm_request_config,
)
```

### Connect and process messages with MCP client

```python
async def main():

    # Establish connection between the client and the server.
    await client.connect_to_server(server_name)

    # messages_in are coming from user interacting with the LLM
    # e.g. UI making use of this MCP client.
    messages_in = ...
    messages_out = await client.process_messages(messages_in)

    # messages_out contains the LLM response. If required, the LLM make use of
    # the available tools offered by the connected servers.
```
