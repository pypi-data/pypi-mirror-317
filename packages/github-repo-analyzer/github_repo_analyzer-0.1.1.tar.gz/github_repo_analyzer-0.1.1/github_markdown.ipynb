{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GitHub Repository Markdown Tool\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/gist/DSamuelHodge/d07a8848fc2094fe69bf242ab4032f2f/github-markdown.ipynb)\n",
        "\n",
        "This notebook provides a powerful tool for analyzing GitHub repositories in markdown, generating detailed structural analysis, and calculating token counts for AI/ML projects.\n",
        "\n",
        "## Features\n",
        "- Generates comprehensive source tree visualization with file sizes and line counts\n",
        "- Calculates total token count using OpenAI's tiktoken library\n",
        "- Provides file type statistics and repository summaries\n",
        "- Parallel processing for faster analysis\n",
        "- Detailed code content examination\n",
        "- Handles multiple programming languages\n",
        "- Excludes common non-code directories (node_modules, __pycache__, etc.)\n",
        "\n",
        "## Quick Start\n",
        "```python\n",
        "from github_analyzer import analyze_github_repo\n",
        "# Markdown a repository\n",
        "analyze_github_repo(\"https://github.com/username/repository\")\n",
        "```\n",
        "\n",
        "## Notes and Limitations\n",
        "- Large repositories may take longer to analyze\n",
        "- Some private repositories may require authentication\n",
        "- Token counts are estimates based on OpenAI's tiktoken library\n",
        "- File size limits prevent processing of very large files (>1MB by default)\n",
        "\n",
        "## Troubleshooting\n",
        "[Click Here](https://colab.research.google.com/drive/15Vb_5YgWMtjNjD9w2yDSTORYeSVqD0vm#scrollTo=NevbU64ZIme6&line=6&uniqifier=1)"
      ],
      "metadata": {
        "id": "KHD-T5VqRamU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "ziQTRpy3K69U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's get started"
      ],
      "metadata": {
        "id": "5nQOERZuKbSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install Dependencies"
      ],
      "metadata": {
        "id": "FT7YodbtnYDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gitpython tiktoken rich\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "print(\"✅ Dependencies installed successfully!\")"
      ],
      "metadata": {
        "id": "7_EPew9ViHc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Import"
      ],
      "metadata": {
        "id": "MMnRp1lUnZ3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import git\n",
        "import logging\n",
        "import json\n",
        "import tempfile\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Set, List, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import tiktoken\n",
        "from git.exc import GitCommandError\n",
        "import re\n",
        "\n",
        "print(\"✅ Github markdown tool imported and ready to use!\")"
      ],
      "metadata": {
        "id": "PMSY3G3BnNQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Insert your the Github repository or subdirectory."
      ],
      "metadata": {
        "id": "fycNoSg93pAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#insert repo or subdirectory\n",
        "github_repo = \"https://github.com/username/repo.git\"\n",
        "\n",
        "print(\"✅ Github repository or subdirectory inserted!\")"
      ],
      "metadata": {
        "id": "8E7MizHhp-3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Extensions include the following:"
      ],
      "metadata": {
        "id": "98RIzlUmnRcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extensions = [\".py\", \".ipynb\", \".js\", \".jsx\", \".ts\", \".tsx\", \".html\", \".css\", \".java\", \".c\", \".cpp\", \".h\", \".cs\", \".rb\", \".php\", \".go\", \".rs\", \".swift\", \".kt\", \".scala\", \".pl\", \".lua\", \".r\", \".sql\", \".sh\", \".bat\", \".m\", \".vb\", \".erl\", \".ex\", \".clj\", \".hs\", \".s\", \".asm\", \".ps1\", \".groovy\", \".f\", \".f90\", \".lisp\", \".lsp\", \".fs\", \".ml\", \".jl\"]\n",
        "\n",
        "print(\"✅ Extensions included!\")"
      ],
      "metadata": {
        "id": "qjjJ4trDnQpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Analysis Configuration"
      ],
      "metadata": {
        "id": "ZXge8pj2pY8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class AnalysisConfig:\n",
        "    \"\"\"Configuration for the analysis process.\"\"\"\n",
        "    extensions: Set[str]\n",
        "    exclude_dirs: Set[str]\n",
        "    max_file_size: int\n",
        "    token_encoding: str\n",
        "    max_workers: int\n",
        "    output_format: str\n",
        "\n",
        "    @classmethod\n",
        "    def get_default_config(cls) -> Dict:\n",
        "        return {\n",
        "            \"extensions\": extensions,\n",
        "            \"exclude_dirs\": [\".git\", \"node_modules\", \"__pycache__\", \".venv\"],\n",
        "            \"max_file_size\": 1024 * 1024,  # 1MB\n",
        "            \"token_encoding\": \"cl100k_base\",\n",
        "            \"max_workers\": 4,\n",
        "            \"output_format\": \"markdown\"\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_file(cls, config_path: Optional[str] = None) -> 'AnalysisConfig':\n",
        "        \"\"\"Load configuration from a JSON file or use defaults.\"\"\"\n",
        "        default_config = cls.get_default_config()\n",
        "\n",
        "        if config_path:\n",
        "            try:\n",
        "                with open(config_path, 'r') as f:\n",
        "                    config = json.load(f)\n",
        "                    config = {**default_config, **config}\n",
        "            except (FileNotFoundError, json.JSONDecodeError) as e:\n",
        "                print(f\"Warning: Could not load config file ({str(e)}). Using defaults.\")\n",
        "                config = default_config\n",
        "        else:\n",
        "            config = default_config\n",
        "\n",
        "        return cls(\n",
        "            extensions=set(config[\"extensions\"]),\n",
        "            exclude_dirs=set(config[\"exclude_dirs\"]),\n",
        "            max_file_size=config[\"max_file_size\"],\n",
        "            token_encoding=config[\"token_encoding\"],\n",
        "            max_workers=config[\"max_workers\"],\n",
        "            output_format=config[\"output_format\"]\n",
        "        )\n",
        "\n",
        "\n",
        "print(\"✅ Analysis configuration dataclass is loaded!\")"
      ],
      "metadata": {
        "id": "clhJXBirpWPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Github Analyzer Code"
      ],
      "metadata": {
        "id": "W3woZcEupffR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GitHubAnalyzer:\n",
        "    def __init__(self, config: AnalysisConfig):\n",
        "        self.config = config\n",
        "        self.token_counter = tiktoken.get_encoding(config.token_encoding)\n",
        "\n",
        "    def parse_github_url(self, github_url: str) -> tuple:\n",
        "        \"\"\"Parse GitHub URL to extract repository URL and subdirectory path.\"\"\"\n",
        "        # Remove 'tree' or 'blob' from URL if present\n",
        "        url_parts = github_url.replace('/tree/', '/').replace('/blob/', '/').split('/')\n",
        "\n",
        "        # Find the main repository URL (everything up to the branch name)\n",
        "        repo_parts = []\n",
        "        branch_found = False\n",
        "        subdirectory_parts = []\n",
        "\n",
        "        for i, part in enumerate(url_parts):\n",
        "            if i < 5:  # Always include the first 5 parts (https://github.com/user/repo)\n",
        "                repo_parts.append(part)\n",
        "            elif not branch_found and '.' not in part and part not in ['main', 'master']:\n",
        "                repo_parts.append(part)\n",
        "            else:\n",
        "                branch_found = True\n",
        "                if i < len(url_parts) - 1:  # Don't include the branch name in subdirectory\n",
        "                    subdirectory_parts.append(part)\n",
        "\n",
        "        repo_url = '/'.join(repo_parts)\n",
        "        subdirectory = '/'.join(subdirectory_parts[1:] if subdirectory_parts else [])\n",
        "\n",
        "        return repo_url, subdirectory\n",
        "\n",
        "    def clone_repository(self, github_url: str, target_dir: str) -> Optional[git.Repo]:\n",
        "        \"\"\"Clone a GitHub repository.\"\"\"\n",
        "        try:\n",
        "            print(f\"Cloning {github_url}...\")\n",
        "            return git.Repo.clone_from(github_url, target_dir)\n",
        "        except GitCommandError as e:\n",
        "            print(f\"Failed to clone repository: {e}\")\n",
        "            return None\n",
        "\n",
        "    def process_file(self, file_path: Path, repo_root: Path) -> Dict:\n",
        "        \"\"\"Process a single file and return its analysis results.\"\"\"\n",
        "        try:\n",
        "            if file_path.stat().st_size > self.config.max_file_size:\n",
        "                print(f\"Skipping large file: {file_path}\")\n",
        "                return {\"skip\": True}\n",
        "\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                content = f.read()\n",
        "                return {\n",
        "                    \"content\": content,\n",
        "                    \"tokens\": len(self.token_counter.encode(content)),\n",
        "                    \"relative_path\": str(file_path.relative_to(repo_root)),\n",
        "                    \"extension\": file_path.suffix,\n",
        "                    \"skip\": False\n",
        "                }\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {file_path}: {e}\")\n",
        "            return {\"skip\": True}\n",
        "\n",
        "    def format_size(self, size_bytes: int) -> str:\n",
        "        \"\"\"Format file size in human readable format.\"\"\"\n",
        "        for unit in ['B', 'KB', 'MB', 'GB']:\n",
        "            if size_bytes < 1024:\n",
        "                return f\"{size_bytes:.1f}{unit}\"\n",
        "            size_bytes /= 1024\n",
        "        return f\"{size_bytes:.1f}GB\"\n",
        "\n",
        "    def count_lines(self, content: str) -> int:\n",
        "        \"\"\"Count non-empty lines in content.\"\"\"\n",
        "        return len([line for line in content.splitlines() if line.strip()])\n",
        "\n",
        "    def analyze_repository(self, github_url: str) -> bool:\n",
        "        \"\"\"Analyze a GitHub repository and generate documentation.\"\"\"\n",
        "        if not github_url:\n",
        "            print(\"No GitHub URL provided.\")\n",
        "            return False\n",
        "\n",
        "        # Parse the GitHub URL to handle subdirectories\n",
        "        repo_url, subdirectory = self.parse_github_url(github_url)\n",
        "        if subdirectory:\n",
        "            print(f\"Analyzing subdirectory: {subdirectory}\")\n",
        "\n",
        "        # Create temporary directory for cloning\n",
        "        with tempfile.TemporaryDirectory() as temp_dir:\n",
        "            repo_path = Path(temp_dir)\n",
        "            repo = self.clone_repository(repo_url, repo_path)\n",
        "\n",
        "            if not repo:\n",
        "                return False\n",
        "\n",
        "            # Determine the path to analyze\n",
        "            analysis_path = repo_path / subdirectory if subdirectory else repo_path\n",
        "            if not analysis_path.exists():\n",
        "                print(f\"Subdirectory {subdirectory} not found in repository\")\n",
        "                return False\n",
        "\n",
        "            # Extract repository/directory name for output\n",
        "            repo_name = subdirectory.split('/')[-1] if subdirectory else repo_url.rstrip(\"/\").split(\"/\")[-1].replace(\".git\", \"\")\n",
        "\n",
        "            # Initialize analysis results\n",
        "            analysis_results = {\n",
        "                \"repository\": repo_name,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"files\": [],\n",
        "                \"total_tokens\": 0\n",
        "            }\n",
        "\n",
        "            # Collect files for processing\n",
        "            files_to_process = []\n",
        "            for root, dirs, files in os.walk(analysis_path):\n",
        "                dirs[:] = [d for d in dirs if d not in self.config.exclude_dirs]\n",
        "\n",
        "                for file in files:\n",
        "                    file_path = Path(root) / file\n",
        "                    if any(file.endswith(ext) for ext in self.config.extensions):\n",
        "                        files_to_process.append(file_path)\n",
        "\n",
        "            print(f\"Processing {len(files_to_process)} files...\")\n",
        "\n",
        "            # Process files in parallel\n",
        "            with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:\n",
        "                futures = [\n",
        "                    executor.submit(self.process_file, file_path, analysis_path)\n",
        "                    for file_path in files_to_process\n",
        "                ]\n",
        "\n",
        "                for future in futures:\n",
        "                    result = future.result()\n",
        "                    if not result[\"skip\"]:\n",
        "                        analysis_results[\"files\"].append(result)\n",
        "                        analysis_results[\"total_tokens\"] += result[\"tokens\"]\n",
        "\n",
        "            # Generate output\n",
        "            output_path = self.generate_output(analysis_results)\n",
        "            print(f\"Analysis completed. Output saved to: {output_path}\")\n",
        "            return True\n",
        "\n",
        "    def generate_output(self, analysis_results: Dict) -> str:\n",
        "        \"\"\"Generate analysis output in the specified format.\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "        output_path = f\"{analysis_results['repository']}_analysis_{timestamp}\"\n",
        "\n",
        "        if self.config.output_format == \"markdown\":\n",
        "            output_path += \".md\"\n",
        "            self.generate_markdown(analysis_results, output_path)\n",
        "        elif self.config.output_format == \"json\":\n",
        "            output_path += \".json\"\n",
        "            self.generate_json(analysis_results, output_path)\n",
        "        else:\n",
        "            print(f\"Unsupported output format: {self.config.output_format}\")\n",
        "            return \"\"\n",
        "\n",
        "        return output_path\n",
        "\n",
        "    def generate_markdown(self, analysis_results: Dict, output_path: str):\n",
        "        \"\"\"Generate Markdown documentation with enhanced source tree.\"\"\"\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            # Repository header\n",
        "            repo_name = analysis_results['repository']\n",
        "            f.write(f\"# {repo_name} Analysis\\n\\n\")\n",
        "\n",
        "            # Summary section\n",
        "            total_files = len(analysis_results['files'])\n",
        "            total_lines = sum(self.count_lines(file['content']) for file in analysis_results['files'])\n",
        "            total_size = sum(len(file['content'].encode('utf-8')) for file in analysis_results['files'])\n",
        "\n",
        "            f.write(\"## Repository Summary\\n\\n\")\n",
        "            f.write(f\"- **Generated:** {analysis_results['timestamp']}\\n\")\n",
        "            f.write(f\"- **Total Files:** {total_files:,}\\n\")\n",
        "            f.write(f\"- **Total Lines:** {total_lines:,}\\n\")\n",
        "            f.write(f\"- **Total Size:** {self.format_size(total_size)}\\n\")\n",
        "            f.write(f\"- **Total Tokens:** {analysis_results['total_tokens']:,}\\n\\n\")\n",
        "\n",
        "            # File type statistics\n",
        "            extension_stats = {}\n",
        "            for file in analysis_results['files']:\n",
        "                ext = file['extension']\n",
        "                if ext not in extension_stats:\n",
        "                    extension_stats[ext] = {'count': 0, 'lines': 0, 'size': 0}\n",
        "                stats = extension_stats[ext]\n",
        "                stats['count'] += 1\n",
        "                stats['lines'] += self.count_lines(file['content'])\n",
        "                stats['size'] += len(file['content'].encode('utf-8'))\n",
        "\n",
        "            f.write(\"## File Type Statistics\\n\\n\")\n",
        "            f.write(\"| Extension | Files | Lines | Size |\\n\")\n",
        "            f.write(\"|-----------|-------|-------|------|\\n\")\n",
        "            for ext, stats in sorted(extension_stats.items(), key=lambda x: x[1]['lines'], reverse=True):\n",
        "                f.write(f\"| {ext or 'no ext'} | {stats['count']:,} | {stats['lines']:,} | {self.format_size(stats['size'])} |\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            # Enhanced source tree\n",
        "            f.write(\"## Source Tree\\n\\n\")\n",
        "            f.write(\"```\\n\")\n",
        "\n",
        "            # Organize files by directory\n",
        "            file_tree = {}\n",
        "            for file_info in analysis_results[\"files\"]:\n",
        "                path_parts = file_info[\"relative_path\"].split(os.sep)\n",
        "                current_dict = file_tree\n",
        "                for part in path_parts[:-1]:\n",
        "                    if part not in current_dict:\n",
        "                        current_dict[part] = {'__files__': []}\n",
        "                    current_dict = current_dict[part]\n",
        "                current_dict['__files__'] = current_dict.get('__files__', [])\n",
        "                current_dict['__files__'].append(file_info)\n",
        "\n",
        "            # Write tree structure with enhanced information\n",
        "            def write_tree(d: Dict, prefix: str = \"\", is_last: bool = True):\n",
        "                entries = sorted([(k, v) for k, v in d.items() if k != '__files__'])\n",
        "                files = d.get('__files__', [])\n",
        "\n",
        "                for idx, (name, content) in enumerate(entries):\n",
        "                    is_last_entry = idx == len(entries) - 1 and not files\n",
        "                    f.write(f\"{prefix}{'└──' if is_last_entry else '├──'} {name}/\\n\")\n",
        "                    new_prefix = prefix + ('    ' if is_last_entry else '│   ')\n",
        "                    write_tree(content, new_prefix, idx == len(entries) - 1)\n",
        "\n",
        "                for idx, file_info in enumerate(sorted(files, key=lambda x: x['relative_path'])):\n",
        "                    is_last_file = idx == len(files) - 1\n",
        "                    name = os.path.basename(file_info['relative_path'])\n",
        "                    lines = self.count_lines(file_info['content'])\n",
        "                    size = len(file_info['content'].encode('utf-8'))\n",
        "                    f.write(f\"{prefix}{'└──' if is_last_file else '├──'} {name:<30} \"\n",
        "                           f\"[{self.format_size(size):>7}, {lines:>4} lines]\\n\")\n",
        "\n",
        "            write_tree(file_tree)\n",
        "            f.write(\"```\\n\\n\")\n",
        "\n",
        "            # File contents\n",
        "            f.write(\"## File Contents\\n\\n\")\n",
        "            for file_info in sorted(analysis_results[\"files\"],\n",
        "                                  key=lambda x: x[\"relative_path\"]):\n",
        "                f.write(f\"### {file_info['relative_path']}\\n\\n\")\n",
        "                f.write(f\"```{file_info['extension'][1:]}\\n\")\n",
        "                f.write(file_info[\"content\"])\n",
        "                f.write(\"\\n```\\n\\n\")\n",
        "\n",
        "    def generate_json(self, analysis_results: Dict, output_path: str):\n",
        "        \"\"\"Generate JSON output.\"\"\"\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(analysis_results, f, indent=2)\n",
        "\n",
        "def analyze_github_repo(github_url: str, config_path: Optional[str] = None):\n",
        "    \"\"\"Analyze a GitHub repository from a notebook.\"\"\"\n",
        "    config = AnalysisConfig.from_file(config_path)\n",
        "    analyzer = GitHubAnalyzer(config)\n",
        "    return analyzer.analyze_repository(github_url)\n",
        "\n",
        "# Batch analysis function\n",
        "def batch_analyze_repos(repo_list: List[str], config_path: Optional[str] = None):\n",
        "    \"\"\"Analyze multiple repositories in batch.\"\"\"\n",
        "    total_repos = len(repo_list)\n",
        "    successful = []\n",
        "    failed = []\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    print(f\"Starting batch analysis of {total_repos} repositories...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for idx, repo_url in enumerate(repo_list, 1):\n",
        "        try:\n",
        "            print(f\"\\n[{idx}/{total_repos}] Analyzing: {repo_url}\")\n",
        "            result = analyze_github_repo(repo_url, config_path)\n",
        "            if result:\n",
        "                successful.append(repo_url)\n",
        "            else:\n",
        "                failed.append((repo_url, \"Analysis failed\"))\n",
        "        except Exception as e:\n",
        "            failed.append((repo_url, str(e)))\n",
        "            print(f\"Error processing {repo_url}: {e}\")\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # Generate summary report\n",
        "    duration = datetime.now() - start_time\n",
        "\n",
        "    print(\"\\nBatch Analysis Summary\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Time taken: {duration}\")\n",
        "    print(f\"Total repositories: {total_repos}\")\n",
        "    print(f\"Successfully analyzed: {len(successful)}\")\n",
        "    print(f\"Failed: {len(failed)}\")\n",
        "\n",
        "    if failed:\n",
        "        print(\"\\nFailed Repositories:\")\n",
        "        for repo, error in failed:\n",
        "            print(f\"- {repo}: {error}\")\n",
        "\n",
        "\n",
        "print(\"✅ Github analyzer code is ready to use!\")"
      ],
      "metadata": {
        "id": "DpVHGlWajr_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. (Optional) Customize Configuration"
      ],
      "metadata": {
        "id": "sfrL6SPwUBsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"extensions\": extensions,\n",
        "    \"exclude_dirs\": [\".git\", \"node_modules\", \"__pycache__\", \".venv\"],\n",
        "    \"max_file_size\": 1048576,\n",
        "    \"token_encoding\": \"cl100k_base\",\n",
        "    \"max_workers\": 4,\n",
        "    \"output_format\": \"markdown\"\n",
        "}\n",
        "\n",
        "with open('config.json', 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"✅ Configuration saved to 'config.json'\")"
      ],
      "metadata": {
        "id": "TmTJ7hClR-bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage Examples\n",
        "\n"
      ],
      "metadata": {
        "id": "NwWxDXQU0wnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Single GitHub Repository Analysis\n",
        "Analyze a complete repository or specific subdirectory:"
      ],
      "metadata": {
        "id": "77u8j4CT5zrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_github_repo(github_repo)"
      ],
      "metadata": {
        "id": "3VzDT4KgnqAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "GztIfF3BLEjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Batch Analysis\n",
        "\n",
        "This code provides several features:\n",
        "\n",
        "1. **Batch Processing:**\n",
        "   - Processes multiple repositories sequentially\n",
        "   - Tracks successful and failed analyses\n",
        "   - Provides progress updates\n",
        "\n",
        "2. **Error Handling:**\n",
        "   - Catches and logs errors for each repository\n",
        "   - Continues processing even if one repository fails\n",
        "   - Provides detailed error messages in the summary\n",
        "\n",
        "3. **Performance Tracking:**\n",
        "   - Measures total execution time\n",
        "   - Shows progress (current/total repositories)\n",
        "   - Generates a summary report\n",
        "\n",
        "4. **Flexibility:**\n",
        "   - Can take a list of repositories directly\n",
        "   - Can read repositories from a text file\n",
        "   - Easy to modify for different input formats\n",
        "\n",
        "You can use it in two ways:\n",
        "\n",
        "1. **Direct list:**\n",
        "```python\n",
        "repos = [\n",
        "    \"https://github.com/repo1\",\n",
        "    \"https://github.com/repo2\",\n",
        "    \"https://github.com/repo3\"\n",
        "]\n",
        "batch_analyze_repos(repos)\n",
        "```\n",
        "\n",
        "2. **From file:**\n",
        "```python\n",
        "# First create the file\n",
        "%%writefile repos.txt\n",
        "https://github.com/repo1\n",
        "https://github.com/repo2\n",
        "https://github.com/repo3\n",
        "# Then analyze\n",
        "repos = read_repos_from_file('repos.txt')\n",
        "batch_analyze_repos(repos)\n",
        "```\n",
        "\n",
        "The output will show progress for each repository and provide a summary at the end with success/failure statistics and timing information."
      ],
      "metadata": {
        "id": "HgqAZVuZ2-gY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1 Direct List**"
      ],
      "metadata": {
        "id": "qJ1Aqf6oFSVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repos_to_analyze = [\n",
        "    \"https://github.com/repo1\",\n",
        "    \"https://github.com/repo2\",\n",
        "    \"https://github.com/repo3\",\n",
        "    # Add more repositories here\n",
        "]\n",
        "\n",
        "batch_analyze_repos(repos_to_analyze)"
      ],
      "metadata": {
        "id": "CnE_CCoB29-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 Read repositories from file**"
      ],
      "metadata": {
        "id": "Mebst6xtC-Vk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First Create the File"
      ],
      "metadata": {
        "id": "AE3TF23jHkkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First create the file\n",
        "%%writefile repos.txt\n",
        "https://github.com/repo1\n",
        "https://github.com/repo2\n",
        "https://github.com/repo3"
      ],
      "metadata": {
        "id": "Umb66KnECLui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the file and analyze the batch repositories"
      ],
      "metadata": {
        "id": "K_J1hiSTHFTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_repos_from_file(filename):\n",
        "    \"\"\"Read repository URLs from a text file.\"\"\"\n",
        "    with open(filename, 'r') as f:\n",
        "        return [line.strip() for line in f if line.strip()]\n",
        "\n",
        "repos = read_repos_from_file('repos.txt')\n",
        "batch_analyze_repos(repos)\n",
        "\n",
        "print(\"✅ Repositories read from file and batch analysis completed!\")"
      ],
      "metadata": {
        "id": "4_qr14bx7EHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "z_QI-ZDALKRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Interactive Analysis"
      ],
      "metadata": {
        "id": "AXS8K7Qj65HI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run and Enter your GitHub Repo URL"
      ],
      "metadata": {
        "id": "BeymNlIA7U0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_analysis():\n",
        "    \"\"\"Interactive repository analysis.\"\"\"\n",
        "    while True:\n",
        "        repo_url = input(\"Enter GitHub repository URL (or 'q' to quit): \").strip()\n",
        "        if repo_url.lower() == 'q':\n",
        "            print(\"Analysis completed.\")\n",
        "            break\n",
        "        if repo_url:\n",
        "            print(f\"\\nAnalyzing repository: {repo_url}\")\n",
        "            analyze_github_repo(repo_url)\n",
        "            print(\"\\n---\\n\")\n",
        "\n",
        "# Example usage:\n",
        "interactive_analysis()"
      ],
      "metadata": {
        "id": "Ev9XTPK267hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Troubleshooting\n",
        "If you encounter issues:\n",
        "1. Check the repository URL format\n",
        "2. Ensure the repository is public\n",
        "3. Verify subdirectory paths if analyzing specific directories\n",
        "4. Check your internet connection for cloning issues"
      ],
      "metadata": {
        "id": "NevbU64ZIme6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Usage Documentation"
      ],
      "metadata": {
        "id": "UJW5ZxhSLh9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_examples():\n",
        "    print(\"Example Usage:\")\n",
        "    print(\"\\n1. Analyze a complete repository:\")\n",
        "    print('analyze_github_repo(\"https://github.com/username/repository\")')\n",
        "\n",
        "    print(\"\\n2. Analyze a specific subdirectory:\")\n",
        "    print('analyze_github_repo(\"https://github.com/username/repository/tree/main/src\")')\n",
        "\n",
        "    print(\"\\n3. Batch analysis:\")\n",
        "    print('repos = [\\n    \"https://github.com/repo1\",\\n    \"https://github.com/repo2\"\\n]')\n",
        "    print('batch_analyze_repos(repos)')\n",
        "\n",
        "# Uncomment to show examples\n",
        "show_examples()"
      ],
      "metadata": {
        "id": "HT-BcqolEFbR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5903b9a5-8634-44e9-d3ba-c93a9476a407"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Usage:\n",
            "\n",
            "1. Analyze a complete repository:\n",
            "analyze_github_repo(\"https://github.com/username/repository\")\n",
            "\n",
            "2. Analyze a specific subdirectory:\n",
            "analyze_github_repo(\"https://github.com/username/repository/tree/main/src\")\n",
            "\n",
            "3. Batch analysis:\n",
            "repos = [\n",
            "    \"https://github.com/repo1\",\n",
            "    \"https://github.com/repo2\"\n",
            "]\n",
            "batch_analyze_repos(repos)\n"
          ]
        }
      ]
    }
  ]
}
