{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEaiCalovMxx",
        "outputId": "fdb272ed-f165-4bfd-e2fe-24c9ca269a96"
      },
      "outputs": [],
      "source": [
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "import os\n",
        "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmJ7EJpZv1K2",
        "outputId": "a98662c9-28f8-4fbc-ea81-9fc557db25d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/vijay/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "INFO:httpx:HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token(s) set successfully\n"
          ]
        }
      ],
      "source": [
        "from ragaai_catalyst import RagaAICatalyst\n",
        "\n",
        "# catalyst = RagaAICatalyst(\n",
        "#     access_key=\"dn5OuL29uA0K6G2046ZG\",\n",
        "#     secret_key=\"Je5Kyg787f4Tdth9c75VBAen198L8KOihMTrhdYB\",\n",
        "#     base_url='http://15.206.202.3/api'\n",
        "#     # api_keys=OPENAI_API_KEY\n",
        "# )\n",
        "# os.environ[\"RAGAAI_CATALYST_BASE_URL\"] = \"http://15.206.202.3/api\"\n",
        "\n",
        "catalyst = RagaAICatalyst(\n",
        "    access_key=\"GOJqDkYz9WHOsJrdnOZq\",\n",
        "    secret_key=\"UkZdlUU733CXoCFXjVrRKisp3OlDjvgevxLU3pWc\",\n",
        "    base_url=\"https://llm-dev5.ragaai.ai/api\"\n",
        "    # api_keys={\"OPENAI_API_KEY\": OPENAI_API_KEY}\n",
        ")\n",
        "\n",
        "# catalyst = RagaAICatalyst(\n",
        "#     access_key=\"jqa9FHN8B313H5mQS14X\",\n",
        "#     secret_key=\"BTzSSKKzFbX8YKoHmIMNpcegsKXF9yRhnYFvyMAF\",\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# project management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Chatbot', 'Text2SQL', 'Q/A', 'Code Generation', 'Others']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "catalyst.project_use_cases()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# catalyst.list_projects()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# project_name = 'chat_demo_sk_v1'\n",
        "project_name = \"prompt_metric_dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# project = catalyst.create_project(\n",
        "#     project_name=project_name,\n",
        "#     usecase=\"Others\" #default usecase Q/A print usecase \n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TravelAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragaai_catalyst import Tracer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer_dataset_name = \"langchain_16Dec_AT_vj\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<ragaai_catalyst.tracers.tracer.Tracer at 0x155e300b0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tracer = Tracer(\n",
        "    project_name=project_name,\n",
        "    dataset_name=tracer_dataset_name,\n",
        "    metadata={\"key1\": \"value1\", \"key2\": \"value2\"},\n",
        "    tracer_type=\"abc\",\n",
        "    pipeline={\n",
        "        \"llm_model\": \"gpt-3.5-turbo\",\n",
        "        \"vector_store\": \"faiss\",\n",
        "        \"embed_model\": \"text-embedding-ada-002\",\n",
        "    }\n",
        ")\n",
        "tracer.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the Personalized Travel Planner!\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracted Preferences:\n",
            "Destination: Karela\n",
            "Activities: Nature\n",
            "Budget: $100\n",
            "Duration (in days): 10\n",
            "\n",
            "Weather in Karela: Weather data not available.\n",
            "Estimated price from delhi to Karela: $500-$1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Planned Itinerary:\n",
            "Day 1-2: Arrival in Karela\n",
            "- Check into a budget-friendly accommodation\n",
            "- Explore the natural beauty of Karela, visit local parks and gardens\n",
            "- Enjoy a peaceful nature walk\n",
            "\n",
            "Day 3-4: Nature Excursions\n",
            "- Take a guided nature tour to explore the pristine forests and wildlife of Karela\n",
            "- Visit waterfalls and scenic viewpoints\n",
            "- Participate in outdoor activities such as hiking or birdwatching\n",
            "\n",
            "Day 5-6: Cultural Experience\n",
            "- Visit local villages and interact with the indigenous communities\n",
            "- Learn about traditional customs and practices\n",
            "- Attend cultural performances and taste local cuisine\n",
            "\n",
            "Day 7-8: Relaxation and Wellness\n",
            "- Pamper yourself with a spa day or wellness retreat\n",
            "- Relax by the beach or a natural hot spring\n",
            "- Practice yoga or meditation in a serene setting\n",
            "\n",
            "Day 9-10: Adventure Activities\n",
            "- Go on a thrilling adventure such as zip-lining, rafting, or jungle trekking\n",
            "- Explore caves or go on a safari to spot exotic wildlife\n",
            "- End your trip with a memorable experience in Karela\n",
            "\n",
            "Overall, this itinerary offers a mix of nature, culture, relaxation, and adventure within your budget of $100. Enjoy your 10-day trip to Karela!\n",
            "\n",
            "Currency conversion not available.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Travel Summary:\n",
            "Destination: Karela\n",
            "Activities: Nature, cultural experiences, relaxation, adventure\n",
            "Budget: $100\n",
            "Duration: 10 days\n",
            "Itinerary: Explore natural beauty, take nature excursions, experience local culture, relax and enjoy wellness activities, participate in adventure activities\n",
            "Flight Price: Estimated $500-$1000 from Delhi to Karela\n",
            "\n",
            "Overall, this travel plan offers a diverse and exciting experience in Karela within a budget of $100 for 10 days. Enjoy a mix of nature, culture, relaxation, and adventure activities during your trip.\n",
            "Trace saved to traces/prompt_metric_dataset_20241216_141734.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from openai import OpenAI\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "import sys\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@tracer.trace_tool(\n",
        "    name=\"llm_call\",\n",
        "    tool_type=\"llm\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "def llm_call(prompt, max_tokens=512, model=\"gpt-3.5-turbo\"):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "@tracer.trace_tool(\n",
        "    name=\"weather_tool\",\n",
        "    tool_type=\"api\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "def weather_tool(destination):\n",
        "    api_key = os.environ.get(\"OPENWEATHERMAP_API_KEY\")\n",
        "    base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n",
        "    params = {\"q\": destination, \"appid\": api_key, \"units\": \"metric\"}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        weather_description = data[\"weather\"][0][\"description\"]\n",
        "        temperature = data[\"main\"][\"temp\"]\n",
        "        return f\"{weather_description.capitalize()}, {temperature:.1f}°C\"\n",
        "    except requests.RequestException:\n",
        "        return \"Weather data not available.\"\n",
        "\n",
        "@tracer.trace_tool(\n",
        "    name=\"currency_converter_tool\",\n",
        "    tool_type=\"api\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "def currency_converter_tool(amount, from_currency, to_currency):\n",
        "    api_key = os.environ.get(\"EXCHANGERATE_API_KEY\")\n",
        "    base_url = f\"https://v6.exchangerate-api.com/v6/{api_key}/pair/{from_currency}/{to_currency}\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if data[\"result\"] == \"success\":\n",
        "            rate = data[\"conversion_rate\"]\n",
        "            return amount * rate\n",
        "        return None\n",
        "    except requests.RequestException:\n",
        "        return None\n",
        "\n",
        "@tracer.trace_tool(\n",
        "    name=\"flight_price_estimator_tool\",\n",
        "    tool_type=\"mock\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "def flight_price_estimator_tool(origin, destination):\n",
        "    return f\"Estimated price from {origin} to {destination}: $500-$1000\"\n",
        "\n",
        "@tracer.trace_agent(\n",
        "    name=\"itinerary_agent\",\n",
        "    agent_type=\"planner\",\n",
        "    capabilities=[\"itinerary_planning\", \"llm_interaction\"]\n",
        ")\n",
        "class ItineraryAgent:\n",
        "    def __init__(self, persona=\"Itinerary Agent\"):\n",
        "        self.persona = persona\n",
        "\n",
        "    def plan_itinerary(self, user_preferences, duration=3):\n",
        "            itinerary_prompt = f\"\"\"\n",
        "You are a travel expert named {self.persona}.\n",
        "Based on the following user preferences, create a {duration}-day travel itinerary.\n",
        "\n",
        "User Preferences:\n",
        "{user_preferences}\n",
        "\n",
        "Itinerary:\n",
        "\"\"\"\n",
        "            return llm_call(itinerary_prompt, max_tokens=512)\n",
        "\n",
        "@tracer.trace_agent(\n",
        "    name=\"travel_agent\",\n",
        "    agent_type=\"orchestrator\",\n",
        "    capabilities=[\"preference_extraction\", \"travel_planning\", \"information_gathering\"]\n",
        ")\n",
        "def travel_agent():\n",
        "    print(\"Welcome to the Personalized Travel Planner!\\n\")\n",
        "\n",
        "    # Get user input\n",
        "    user_input = \"karela, 10 days, $100, nature\"\n",
        "\n",
        "    # Extract preferences\n",
        "    preferences_prompt = f\"\"\"\n",
        "Extract key travel preferences from the following user input:\n",
        "\"{user_input}\"\n",
        "\n",
        "Please provide the extracted information in this format:\n",
        "Destination:\n",
        "Activities:\n",
        "Budget:\n",
        "Duration (in days):\n",
        "\"\"\"\n",
        "    extracted_preferences = llm_call(preferences_prompt)\n",
        "    print(\"\\nExtracted Preferences:\")\n",
        "    print(extracted_preferences)\n",
        "\n",
        "    # Parse extracted preferences\n",
        "    preferences = {}\n",
        "    for line in extracted_preferences.split(\"\\n\"):\n",
        "        if \":\" in line:\n",
        "            key, value = line.split(\":\", 1)\n",
        "            preferences[key.strip()] = value.strip()\n",
        "\n",
        "    # Validate extracted preferences\n",
        "    required_keys = [\"Destination\", \"Activities\", \"Budget\", \"Duration (in days)\"]\n",
        "    if not all(key in preferences for key in required_keys):\n",
        "        print(\"\\nCould not extract all required preferences. Please try again.\")\n",
        "        return\n",
        "\n",
        "    # Fetch additional information\n",
        "    weather = weather_tool(preferences[\"Destination\"])\n",
        "    print(f\"\\nWeather in {preferences['Destination']}: {weather}\")\n",
        "\n",
        "    origin = \"delhi\"\n",
        "    flight_price = flight_price_estimator_tool(origin, preferences[\"Destination\"])\n",
        "    print(flight_price)\n",
        "\n",
        "    # Plan itinerary\n",
        "    itinerary_agent = ItineraryAgent()\n",
        "    itinerary = itinerary_agent.plan_itinerary(\n",
        "        extracted_preferences, int(preferences[\"Duration (in days)\"])\n",
        "    )\n",
        "    print(\"\\nPlanned Itinerary:\")\n",
        "    print(itinerary)\n",
        "\n",
        "    # Currency conversion\n",
        "    budget_amount = float(preferences[\"Budget\"].replace(\"$\", \"\").replace(\",\", \"\"))\n",
        "    converted_budget = currency_converter_tool(budget_amount, \"USD\", \"INR\")\n",
        "    if converted_budget:\n",
        "        print(f\"\\nBudget in INR: {converted_budget:.2f} INR\")\n",
        "    else:\n",
        "        print(\"\\nCurrency conversion not available.\")\n",
        "\n",
        "    # Generate travel summary\n",
        "    summary_prompt = f\"\"\"\n",
        "Summarize the following travel plan:\n",
        "\n",
        "Destination: {preferences['Destination']}\n",
        "Activities: {preferences['Activities']}\n",
        "Budget: {preferences['Budget']}\n",
        "Duration: {preferences['Duration (in days)']} days\n",
        "Itinerary: {itinerary}\n",
        "Weather: {weather}\n",
        "Flight Price: {flight_price}\n",
        "\n",
        "Travel Summary:\n",
        "\"\"\"\n",
        "    travel_summary = llm_call(summary_prompt, max_tokens=2048)\n",
        "    print(\"\\nTravel Summary:\")\n",
        "    print(travel_summary)\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        travel_agent()\n",
        "    finally:\n",
        "        # Stop tracing and save results\n",
        "        tracer.stop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tracer management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### start tracer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragaai_catalyst import Tracer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer_dataset_name = \"langchain_16Dec_AT_vj\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'prompt_metric_dataset'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "project_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<ragaai_catalyst.tracers.tracer.Tracer at 0x155eeccb0>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tracer = Tracer(\n",
        "    project_name=project_name,\n",
        "    dataset_name=tracer_dataset_name,\n",
        "    metadata={\"key1\": \"value1\", \"key2\": \"value2\"},\n",
        "    tracer_type=\"abc\",\n",
        "    pipeline={\n",
        "        \"llm_model\": \"gpt-3.5-turbo\",\n",
        "        \"vector_store\": \"faiss\",\n",
        "        \"embed_model\": \"text-embedding-ada-002\",\n",
        "    }\n",
        ")\n",
        "tracer.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### langchain ex1 sdk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred while loading the document: File path /Users/siddharthakosti/Downloads/catalyst_error_handling/catalyst_v2/catalyst_v2_new_1/data/ai_document_061023_2.pdf is not a valid file or url\n",
            "An error occurred while loading the document: File path /Users/siddharthakosti/Downloads/catalyst_error_handling/catalyst_v2/catalyst_v2_new_1/data/ai_document_061023_2.pdf is not a valid file or url\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-openai langchain-chroma langchain-community pypdf -q\n",
        "\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from opentelemetry.trace import SpanKind\n",
        "source_doc_path = \"/Users/siddharthakosti/Downloads/catalyst_error_handling/catalyst_v2/catalyst_v2_new_1/data/ai_document_061023_2.pdf\"\n",
        "\n",
        "# Initialize necessary variables\n",
        "retriever = None\n",
        "loaded_doc = None\n",
        "def load_document(source_doc_path):\n",
        "\n",
        "    try:\n",
        "        loader = PyPDFLoader(source_doc_path)\n",
        "        pages = loader.load_and_split()\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "        vectorstore = Chroma.from_documents(pages, embeddings)\n",
        "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "        print(\"Document loaded and processed.\")\n",
        "        return retriever\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the document: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_response(retriever, query):\n",
        "\n",
        "    try:\n",
        "        # llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "        template = \"\"\"\n",
        "            You are a helpful AI assistant. Answer based on the context provided.\n",
        "            context: {context}\n",
        "            input: {input}\n",
        "            answer:\n",
        "            \"\"\"\n",
        "        prompt = PromptTemplate.from_template(template)\n",
        "        combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
        "        retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
        "        response = retrieval_chain.invoke({\"input\": query})\n",
        "        print(response[\"answer\"])\n",
        "        return response[\"answer\"]\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while generating the response: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_document(source_doc_path, loaded_doc, query):\n",
        "    try:\n",
        "        if loaded_doc != source_doc_path:\n",
        "            retriever = load_document(source_doc_path)\n",
        "            if retriever is None:\n",
        "                return \"Failed to load document.\"\n",
        "            loaded_doc = source_doc_path\n",
        "        else:\n",
        "            print(\"Using cached document retriever.\")\n",
        "        response = generate_response(retriever, query)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(f\"An overall error occurred: {e}\")\n",
        "        return \"An error occurred during the document processing.\"\n",
        "\n",
        "query = \"What paper targets to solve, tell in 20 words?\"\n",
        "response = process_document(source_doc_path, loaded_doc, query)\n",
        "\n",
        "query = \"What paper aims to resolve, tell in 20 words?\"\n",
        "response = process_document(source_doc_path, loaded_doc, query)\n",
        "\n",
        "# query = \"What is this paper about?\"\n",
        "# # Process the document and get the response\n",
        "# response = process_document(source_doc_path, loaded_doc, query)\n",
        "\n",
        "# query = \"What is the domain of paper?\"\n",
        "# # Process the document and get the response\n",
        "# response = process_document(source_doc_path, loaded_doc, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### langchain ex 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/84/hcqz7gk15dl9tr2fm9ggqyxw0000gp/T/ipykernel_8120/1006200619.py:11: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(temperature=0)\n",
            "/var/folders/84/hcqz7gk15dl9tr2fm9ggqyxw0000gp/T/ipykernel_8120/1006200619.py:20: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt)\n",
            "/var/folders/84/hcqz7gk15dl9tr2fm9ggqyxw0000gp/T/ipykernel_8120/1006200619.py:24: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = chain.run(question)\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The capital of France is Paris.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize the chat model\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "# Create a simple prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Question: {question}\\nAnswer: \"\n",
        ")\n",
        "\n",
        "# Create the chain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Ask a question\n",
        "question = \"What is the capital of France?\"\n",
        "response = chain.run(question)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### langchain ex3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/84/hcqz7gk15dl9tr2fm9ggqyxw0000gp/T/ipykernel_8120/424656271.py:11: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
            "  llm = OpenAI(temperature=0)\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The capital of France is Paris.\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize the language model\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "# Create a simple prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Question: {question}\\nAnswer: \"\n",
        ")\n",
        "\n",
        "# Create the chain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Ask a question\n",
        "question = \"What is the capital of France?\"\n",
        "response = chain.run(question)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### langchian ex 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "str expected, not NoneType",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLANGCHAIN_TRACING_V2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m LANGSMITH_API_KEY \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLANGSMITH_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLANGSMITH_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m LANGSMITH_API_KEY\n",
            "File \u001b[0;32m<frozen os>:690\u001b[0m, in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n",
            "File \u001b[0;32m<frozen os>:764\u001b[0m, in \u001b[0;36mencode\u001b[0;34m(value)\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: str expected, not NoneType"
          ]
        }
      ],
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = LANGSMITH_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load, chunk and index the contents of the blog.\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "\n",
        "# Retrieve and generate using the relevant snippets of the blog.\n",
        "retriever = vectorstore.as_retriever()\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TI code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "# import langchain\n",
        "\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "from opentelemetry.trace import SpanKind\n",
        "\n",
        "from ragaai_catalyst import RagaAICatalyst, Experiment, Dataset, Tracer, Evaluation\n",
        "\n",
        " \n",
        "\n",
        "# langchain.verbose=False\n",
        "\n",
        "# langchain.debug=False\n",
        "\n",
        "source_doc_path = \"/Users/siddharthakosti/Downloads/catalyst_error_handling/catalyst_v2/catalyst_v2_new_1/data/data.pdf\"\n",
        "\n",
        " \n",
        "\n",
        "catalyst = RagaAICatalyst(\n",
        "\n",
        "    access_key=\"pBxij88919zIMggB4T2J\",\n",
        "\n",
        "    secret_key=\"JcTfpL9ARpLH2RdSZqov8K1KyYonADKPbbi02k2k\",\n",
        "\n",
        "    base_url=\"https://catalyst.raga.ai/api\"\n",
        "\n",
        ")\n",
        "\n",
        " \n",
        "\n",
        "tracer = Tracer(\n",
        "\n",
        "            project_name=\"prompt_metric_dataset\",\n",
        "\n",
        "            dataset_name=\"nkb-test-dataset-name\",\n",
        "\n",
        "            metadata={\"key1\": \"value1\", \"key2\": \"value2\"},\n",
        "\n",
        "            tracer_type=\"langchain\",\n",
        "\n",
        "            pipeline={\n",
        "\n",
        "                \"llm_model\": \"gpt-3.5-turbo\",\n",
        "\n",
        "                \"vector_store\": \"faiss\",\n",
        "\n",
        "                \"embed_model\": \"text-embedding-ada-002\",\n",
        "\n",
        "            }\n",
        "\n",
        "        ).start()\n",
        "\n",
        " \n",
        "\n",
        "# Initialize necessary variables\n",
        "\n",
        "retriever = None\n",
        "\n",
        "loaded_doc = None\n",
        "\n",
        "def load_document(source_doc_path):\n",
        "\n",
        " \n",
        "\n",
        "    try:\n",
        "\n",
        "        loader = PyPDFLoader(source_doc_path)\n",
        "\n",
        "        pages = loader.load_and_split()\n",
        "\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "\n",
        "        vectorstore = Chroma.from_documents(pages, embeddings)\n",
        "\n",
        "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "        print(\"Document loaded and processed.\")\n",
        "\n",
        "        return retriever\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        print(f\"An error occurred while loading the document: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        " \n",
        "\n",
        "def generate_response(retriever, query):\n",
        "\n",
        " \n",
        "\n",
        "    try:\n",
        "\n",
        "        # llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "\n",
        "        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "        template = \"\"\"\n",
        "\n",
        "            You are a helpful AI assistant. Answer based on the context provided.\n",
        "\n",
        "            context: {context}\n",
        "\n",
        "            input: {input}\n",
        "\n",
        "            answer:\n",
        "\n",
        "            \"\"\"\n",
        "\n",
        "        prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "        combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "        retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
        "\n",
        "        response = retrieval_chain.invoke({\"input\": query})\n",
        "\n",
        "        print(response[\"answer\"])\n",
        "\n",
        "        return response[\"answer\"]\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        print(f\"An error occurred while generating the response: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        " \n",
        "\n",
        "def process_document(source_doc_path, loaded_doc, query):\n",
        "\n",
        "    try:\n",
        "\n",
        "        if loaded_doc != source_doc_path:\n",
        "\n",
        "            retriever = load_document(source_doc_path)\n",
        "\n",
        "            if retriever is None:\n",
        "\n",
        "                return \"Failed to load document.\"\n",
        "\n",
        "            loaded_doc = source_doc_path\n",
        "\n",
        "        else:\n",
        "\n",
        "            print(\"Using cached document retriever.\")\n",
        "\n",
        "        response = generate_response(retriever, query)\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        print(f\"An overall error occurred: {e}\")\n",
        "\n",
        "        return \"An error occurred during the document processing.\"\n",
        "\n",
        "query = \"Tell me about the paper.\"\n",
        "\n",
        " \n",
        "\n",
        "# Process the document and get the response\n",
        "\n",
        "response = process_document(source_doc_path, loaded_doc, query)\n",
        "\n",
        " \n",
        "\n",
        "query = \"What is the domain of the paper?\"\n",
        "\n",
        " \n",
        "\n",
        "# Process the document and get the response\n",
        "\n",
        "response = process_document(source_doc_path, loaded_doc, query)\n",
        "\n",
        " \n",
        "\n",
        "tracer.stop()\n",
        "\n",
        "tracer.get_upload_status()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### stop tracer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer.get_upload_status()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### langchain-extraction json check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from extraction_logic_langchain import extract_final_trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "with open(\"/Users/siddharthakosti/Downloads/catalyst_error_handling/catalyst_v2/catalyst_v2_new_1/ragaai-catalyst/tracer_debug_app4.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "extract_final_trace(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extract_final_trace_new(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install langchain-openai langchain-chroma langchain-community pypdf -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "# from langchain_chroma import Chroma\n",
        "# # from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# from langchain_community.document_loaders import PyPDFLoader\n",
        "# from langchain.prompts import PromptTemplate\n",
        "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "# from langchain.chains import create_retrieval_chain\n",
        "# from opentelemetry.trace import SpanKind\n",
        "# source_doc_path = \"/Users/siddharthakosti/Downloads/catalyst_error_handling/catalyst_v2/ai document_061023_2.pdf\"\n",
        "\n",
        "# # Initialize necessary variables\n",
        "# retriever = None\n",
        "# loaded_doc = None\n",
        "# def load_document(source_doc_path):\n",
        "\n",
        "#     try:\n",
        "#         loader = PyPDFLoader(source_doc_path)\n",
        "#         pages = loader.load_and_split()\n",
        "#         embeddings = OpenAIEmbeddings()\n",
        "#         vectorstore = Chroma.from_documents(pages, embeddings)\n",
        "#         retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "#         print(\"Document loaded and processed.\")\n",
        "#         return retriever\n",
        "#     except Exception as e:\n",
        "#         print(f\"An error occurred while loading the document: {e}\")\n",
        "#         return None\n",
        "\n",
        "# def generate_response(retriever, query):\n",
        "\n",
        "#     try:\n",
        "#         # llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "#         llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "#         template = \"\"\"\n",
        "#             You are a helpful AI assistant. Answer based on the context provided.\n",
        "#             context: {context}\n",
        "#             input: {input}\n",
        "#             answer:\n",
        "#             \"\"\"\n",
        "#         prompt = PromptTemplate.from_template(template)\n",
        "#         combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
        "#         retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
        "#         response = retrieval_chain.invoke({\"input\": query})\n",
        "#         print(response[\"answer\"])\n",
        "#         return response[\"answer\"]\n",
        "#     except Exception as e:\n",
        "#         print(f\"An error occurred while generating the response: {e}\")\n",
        "#         return None\n",
        "\n",
        "# def process_document(source_doc_path, loaded_doc, query):\n",
        "#     try:\n",
        "#         if loaded_doc != source_doc_path:\n",
        "#             retriever = load_document(source_doc_path)\n",
        "#             if retriever is None:\n",
        "#                 return \"Failed to load document.\"\n",
        "#             loaded_doc = source_doc_path\n",
        "#         else:\n",
        "#             print(\"Using cached document retriever.\")\n",
        "#         response = generate_response(retriever, query)\n",
        "#         return response\n",
        "#     except Exception as e:\n",
        "#         print(f\"An overall error occurred: {e}\")\n",
        "#         return \"An error occurred during the document processing.\"\n",
        "\n",
        "# query = \"Tell me about the paper.\"\n",
        "# response = process_document(source_doc_path, loaded_doc, query)\n",
        "\n",
        "# query = \"What is the domain of the paper?\"\n",
        "# response = process_document(source_doc_path, loaded_doc, query)\n",
        "\n",
        "# query = \"What is the issue addressed in the paper?\"\n",
        "# response = process_document(source_doc_path, loaded_doc, query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tracer.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer.get_upload_status()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## llama-index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragaai_catalyst import Tracer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer_dataset_name = \"llamaI_16_dec_v1\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer = Tracer(\n",
        "    project_name=project_name,\n",
        "    dataset_name=tracer_dataset_name,\n",
        "    metadata={\"key1\": \"value1\", \"key2\": \"value2\"},\n",
        "    tracer_type=\"llamaindex\",\n",
        "    pipeline={\n",
        "        \"llm_model\": \"gpt-3.5-turbo\",\n",
        "        \"vector_store\": \"faiss\",\n",
        "        \"embed_model\": \"text-embedding-ada-002\",\n",
        "    }\n",
        ").start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### app 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import Settings, StorageContext, VectorStoreIndex\n",
        "from llama_index.readers.file import PDFReader\n",
        "from llama_index.core import SimpleDirectoryReader, Settings, StorageContext, VectorStoreIndex\n",
        "# Load a PDF document\n",
        "documents = SimpleDirectoryReader(\"/Users/siddharthakosti/Downloads/catalyst_error_handling/catalyst_v2/catalyst_v2_new/data\").load_data()\n",
        "\n",
        "Settings.chunk_size = 1024\n",
        "nodes = Settings.node_parser.get_nodes_from_documents(documents)\n",
        "storage_context = StorageContext.from_defaults()\n",
        "\n",
        "summary_index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
        "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
        "\n",
        "# Querying the indices\n",
        "list_query_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\", use_async=True)\n",
        "vector_query_engine = vector_index.as_query_engine(response_mode=\"tree_summarize\", use_async=True)\n",
        "\n",
        "response = list_query_engine.query(\"Why is customer service training important??\")\n",
        "response = list_query_engine.query(\"What is influences of the driving direction within the calculation of the pitting load-carrying capacity of bevel and hypoid gears?\")\n",
        "# response = list_query_engine.query(\"Where school district 1401 is located?\")\n",
        "# response = list_query_engine.query(\"Alan Turing definition fall under the category of?\")\n",
        "# print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### app1: sdk example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, ServiceContext, Document\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.readers.file import PDFReader\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from typing import Any, Dict, List, Optional\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "# Initialize necessary variables\n",
        "retriever = None\n",
        "loaded_doc = None\n",
        "index = None\n",
        "\n",
        "def load_document(source_doc_path):\n",
        "    \"\"\"\n",
        "    Load and index the document using LlamaIndex\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize LLM and embedding model\n",
        "        llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "        embed_model = OpenAIEmbedding()\n",
        "        \n",
        "        # Create service context\n",
        "        service_context = ServiceContext.from_defaults(\n",
        "            llm=llm,\n",
        "            embed_model=embed_model,\n",
        "        )\n",
        "        \n",
        "        # Load PDF document\n",
        "        reader = PDFReader()\n",
        "        docs = reader.load_data(source_doc_path)\n",
        "        \n",
        "        # Create documents with metadata\n",
        "        documents = [\n",
        "            Document(text=doc.text, metadata={\"source\": source_doc_path})\n",
        "            for doc in docs\n",
        "        ]\n",
        "        \n",
        "        # Create vector store index\n",
        "        global index\n",
        "        index = VectorStoreIndex.from_documents(\n",
        "            documents,\n",
        "            service_context=service_context\n",
        "        )\n",
        "        \n",
        "        # Create retriever (to maintain similar interface)\n",
        "        retriever = index.as_retriever(similarity_top_k=5)\n",
        "        \n",
        "        logger.info(\"Document loaded and processed.\")\n",
        "        return retriever\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred while loading the document: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_response(retriever, query):\n",
        "    \"\"\"\n",
        "    Generate response for the given query using LlamaIndex\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if index is None:\n",
        "            logger.error(\"Index not initialized. Please load document first.\")\n",
        "            return None\n",
        "            \n",
        "        # Initialize LLM with callbacks\n",
        "        llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "        \n",
        "        # Create service context with callback manager\n",
        "        service_context = ServiceContext.from_defaults(\n",
        "            llm=llm\n",
        "        )\n",
        "        \n",
        "        # Create query engine\n",
        "        query_engine = index.as_query_engine(\n",
        "            response_mode=\"compact\",\n",
        "            service_context=service_context\n",
        "        )\n",
        "        \n",
        "        # Generate response\n",
        "        response = query_engine.query(query)\n",
        "        \n",
        "        logger.info(\"Response generated successfully\")\n",
        "        return str(response)\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred while generating the response: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_document(source_doc_path, loaded_doc, query):\n",
        "    \"\"\"\n",
        "    Process document and generate response using LlamaIndex\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if we need to load a new document\n",
        "        if loaded_doc != source_doc_path:\n",
        "            retriever = load_document(source_doc_path)\n",
        "            if retriever is None:\n",
        "                return \"Failed to load document.\"\n",
        "            loaded_doc = source_doc_path\n",
        "        else:\n",
        "            logger.info(\"Using cached document retriever.\")\n",
        "        \n",
        "        # Generate response\n",
        "        response = generate_response(retriever, query)\n",
        "        if response is None:\n",
        "            return \"Failed to generate response.\"\n",
        "            \n",
        "        return response\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"An overall error occurred: {e}\")\n",
        "        return \"An error occurred during the document processing.\"\n",
        "    \n",
        "\n",
        "\n",
        "source_doc_path = \"/Users/siddharthakosti/Downloads/langchain_callbacks/llama-index/ai document_061023_2.pdf\"\n",
        "\n",
        "# Process a query\n",
        "query = \"What is this paper about?\"\n",
        "response = process_document(source_doc_path, None, query)\n",
        "print(f\"Response: {response}\")\n",
        "\n",
        "query = \"Give 20 words summary of the paper?\"\n",
        "response = process_document(source_doc_path, None, query)\n",
        "print(f\"Response: {response}\")\n",
        "\n",
        "query = \"What is the main topic of the paper?\"\n",
        "response = process_document(source_doc_path, None, query)\n",
        "print(f\"Response: {response}\")\n",
        "\n",
        "query = \"What is the aim of the paper?\"\n",
        "response = process_document(source_doc_path, None, query)\n",
        "print(f\"Response: {response}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### app4 only openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "print(OpenAI.complete) \n",
        "llm = OpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    api_key=OPENAI_API_KEY,  # uses OPENAI_API_KEY env var by default\n",
        ")\n",
        "\n",
        "resp = llm.complete(\"Paul Graham is \")\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### app 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/Users/siddharthakosti/Downloads/catalyst_error_handling/catalyst_v2/catalyst_v2_new_1/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### app5: gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install llama-index-llms-gemini llama-index -q\n",
        "\n",
        "import os\n",
        "\n",
        "GOOGLE_API_KEY = \"\"  # add your GOOGLE API key here\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "from llama_index.llms.gemini import Gemini\n",
        "\n",
        "llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    # api_key=\"some key\",  # uses GOOGLE_API_KEY env var by default\n",
        ")\n",
        "\n",
        "resp = llm.complete(\"Write a poem about a magic backpack\")\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### app6: Groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install llama-index-llms-groq llama-index -q\n",
        "\n",
        "from llama_index.llms.groq import Groq\n",
        "\n",
        "llm = Groq(model=\"llama3-70b-8192\", api_key=\"your_api_key\")\n",
        "\n",
        "response = llm.complete(\"Explain the importance of low latency LLMs\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### app7: Cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install llama-index-llms-openai llama-index-llms-cohere llama-index -q\n",
        "\n",
        "from llama_index.llms.cohere import Cohere\n",
        "\n",
        "api_key = \"Your api key\"\n",
        "resp = Cohere(api_key=api_key).complete(\"Paul Graham is \")\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer.get_upload_status()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### llama-index extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from extraction_logic_llama_index import extract_llama_index_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "with open(\"/Users/siddharthakosti/Downloads/catalyst_error_handling/catalyst_v2/catalyst_v2_new_1/ragaai-catalyst/saved_trace_query_2_20241121_141851.json\", 'r') as f:\n",
        "    data = json.load(f)\n",
        "extract_llama_index_data(data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "project_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer_dataset_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragaai_catalyst import Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation = Evaluation(project_name=project_name, \n",
        "                        dataset_name=tracer_dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation.list_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics=[\n",
        "        {\"name\": \"Hallucination\", \"config\": {\"model\": \"gpt-4o-mini\", \"provider\":\"OpenAI\"}, \"column_name\":\"Hallucination_v1\"},\n",
        "        {\"name\": \"Faithfulness\", \"config\": {\"model\": \"gpt-4o-mini\", \"provider\":\"OpenAI\"}, \"column_name\":\"Faithfulness_v1\"}\n",
        "        ]\n",
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation.add_metrics(metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation.get_status()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation.get_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHZinSmzwH7d"
      },
      "source": [
        "# prompt slug error handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RswMU4rHv_2j"
      },
      "outputs": [],
      "source": [
        "from ragaai_catalyst import PromptManager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJUqRltQxeXt"
      },
      "source": [
        "## project name not found error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "6_9C-ONswL13",
        "outputId": "e8a12b36-5d90-4cfc-f532-b9ad06526888"
      },
      "outputs": [],
      "source": [
        "prompt_manager = PromptManager(\"prompt_metric_dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## list prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.list_prompts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFLkT_xyxiPQ"
      },
      "source": [
        "## prompt not found error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "Jb0CNzfAwRcI",
        "outputId": "1ae41f12-7af9-475c-b99e-92ad5acdbdab"
      },
      "outputs": [],
      "source": [
        "prompt_manager.list_prompt_versions(prompt_name=\"hall\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## list prompt versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.list_prompt_versions(prompt_name=\"Hallu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAgiSc0YxliJ"
      },
      "source": [
        "## prompt version not found error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.get_prompt(prompt_name=\"hall\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.get_prompt(version=\"v8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.get_prompt(prompt_name=\"Hallu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "mVkbArG5wTXD",
        "outputId": "75197c40-e91b-4732-d705-d41c979b34a9"
      },
      "outputs": [],
      "source": [
        "prompt = prompt_manager.get_prompt(prompt_name=\"newapiv1\")\n",
        "prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt.get_variables()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt.get_model_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "/Users/siddharthakosti/Downloads/catalyst_error_handling/catalyst_v2/catalyst_v2_new_1/catalyst_2.0.6_release_notes_sample.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt.get_prompt_content()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9ro-FWHxoDn"
      },
      "source": [
        "## error due to missing prompt variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt.compile(\n",
        "    query='What is the capital of France?',\n",
        "    llm_response=\"The capital of France is Paris.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_eBPjlsxx_0"
      },
      "source": [
        "## error due to incorrect variable type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt.compile(\n",
        "    query='What is the capital of France?',\n",
        "    llm_response=\"The capital of France is Paris.\",\n",
        "    context=123\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yckgj1jxtcC"
      },
      "source": [
        "## error due to extra variable provided"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt.compile(\n",
        "    query='What is the capital of France?',\n",
        "    llm_response=\"The capital of France is Paris.\",\n",
        "    context=\"France is a country in Western Europe. Its capital and largest city is Paris.\",\n",
        "    expected_response= \"ABC\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compiled_prompt = prompt.compile(\n",
        "    query='What is the capital of France?',\n",
        "    llm_response=\"The capital of France is Paris.\",\n",
        "    context=\"France is a country in Western Europe. Its capital and largest city is Paris.\")\n",
        "compiled_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to call OpenAI API\n",
        "import openai\n",
        "def get_openai_response(prompt):\n",
        "    client = openai.OpenAI()\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=prompt\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_openai_response(compiled_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## add new prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "import os\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "from ragaai_catalyst import RagaAICatalyst\n",
        "\n",
        "# catalyst = RagaAICatalyst(\n",
        "#     access_key=\"dn5OuL29uA0K6G2046ZG\",\n",
        "#     secret_key=\"Je5Kyg787f4Tdth9c75VBAen198L8KOihMTrhdYB\",\n",
        "#     base_url='http://15.206.202.3/api'\n",
        "#     # api_keys=OPENAI_API_KEY\n",
        "# )\n",
        "# os.environ[\"RAGAAI_CATALYST_BASE_URL\"] = \"http://15.206.202.3/api\"\n",
        "\n",
        "catalyst = RagaAICatalyst(\n",
        "    access_key=\"vTVxbWhzNq053gXX4lX2\",\n",
        "    secret_key=\"Mz39sOAqop4WDxKvzCxExpXN4N1g0WUp6m6N8xvo\",\n",
        "    api_keys={\"OPENAI_API_KEY\": OPENAI_API_KEY}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragaai_catalyst import PromptManager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "project_name = \"final_test_v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragaai_catalyst import PromptManager\n",
        "prompt_manager = PromptManager(project_name=project_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.list_prompts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.list_prompt_versions(prompt_name=\"Hallu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_name = \"new_prompt_1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# content = prompt_manager.get_prompt(\"Hallu\").get_prompt_content()\n",
        "content = [{'content': 'You are an LLM evaluation tool. Given any Prompt, Context and Response you identify information and metrics such as Hallucination in response. You self check at least 5 times to make sure you are correct',\n",
        "  'role': 'system'},\n",
        " {'content': 'This is the context: {{context}}\\n           This is LLM Response: {{llm_response}}\\n           This is user query: {{query}}\\n\\n           You have to check if the LLM Response is hallucinating wrt to user query and context.\\n           LLM Response should be based on facts present only in Context.\\n           New claims, wrong information, made-up information, or typical correct answers but not present in context should be called hallucination.\\n           Verify 5 times before responding.\\n           Answer in True if it is hallucinationg or False otherwise and give descriptive reason why you believe this is the answer.\\n           Also mention reason_type if it is due to Contradictory wrt context, Incorrect response wrt query, Absence from context.\\n\\n           Return the answer in JSON format without any extra text.\\n',\n",
        "  'role': 'user'}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.add_new_prompt(prompt_name, content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "content = [{'content': 'text 2', 'role': 'system'},\n",
        "           {'content': 'text 2','role': 'user'}]\n",
        "content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.add_new_prompt(prompt_name, content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.add_new_prompt(prompt_name, content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.add_new_prompt(\"prompt_system\", [{'content': '', 'role': 'system'}])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.add_new_prompt(\"prompt_user\", [{'content': '', 'role': 'user'}])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.add_new_prompt(\"prompt_system_w_ar\", [{'content': '{abc} {{def}} {{defaul}}', 'role': 'system'}])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_content = [\n",
        "  {\n",
        "    \"content\": \"adding prompt system variable {{abc}} {{asd}}\",\n",
        "    \"role\": \"system\"\n",
        "  },\n",
        "  {\n",
        "    \"content\": \"adding prompt user variable {{abc}} {{def}}\",\n",
        "    \"role\": \"user\"\n",
        "  },\n",
        "  {\n",
        "    \"content\": \"this is assistant {{abc}} {{ghi}}\",\n",
        "    \"role\": \"assistant\"\n",
        "  }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "content_str = [item[\"content\"] for item in new_content]\n",
        "content_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "content_str = \". \".join(content_str)\n",
        "content_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[\n",
        "  {\n",
        "    \"content\": \" system content\",\n",
        "    \"role\": \"system\"\n",
        "  },\n",
        "  {\n",
        "    \"content\": \"user content\",\n",
        "    \"role\": \"user\"\n",
        "  }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.add_new_prompt(prompt_name+'b', new_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## APO requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  Initial prompt (in single string)\n",
        "2.  Task description (in single string)\n",
        "3.  model \n",
        "4.  api_base\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Guardrails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "import os\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragaai_catalyst import RagaAICatalyst\n",
        "catalyst = RagaAICatalyst(\n",
        "    access_key=\"ypajeHE5e7Nh2O51V0ep\",\n",
        "    secret_key=\"R2ukns5DkA2rylrgJXVGSPiZ5D5Y9i7eSMD6CJn6\",\n",
        "    base_url=\"http://65.2.138.219/api\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "catalyst.list_projects()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "project_name = \"Test_SJ\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragaai_catalyst import GuardrailsManager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gdm = GuardrailsManager(project_name=project_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gdm.list_deployment_ids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gdm.get_deployment(70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gdm.list_guardrails()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gdm.list_fail_condition()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gdm.create_deployment(\"sk-sdk-v4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gdm.list_deployment_ids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "guardrails_config = {\"guardrailFailConditions\": [\"TIMEOUT\"],\n",
        "                     \"deploymentFailCondition\": \"ALL_FAIL\",\n",
        "                     \"alternateResponse\": \"This is alternate\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "guardrails = [\n",
        "    {\n",
        "      \"name\": \"pii_lte\",\n",
        "      \"type\": \"PII\",\n",
        "      \"threshold\": {\n",
        "        \"lte\": 0.23\n",
        "      },\n",
        "      \"isHighRisk\": True,\n",
        "      \"isActive\": True\n",
        "    },\n",
        "    {\n",
        "      \"name\": \"pii_gte\",\n",
        "      \"type\": \"PII\",\n",
        "      \"threshold\": {\n",
        "        \"gte\": 0.23\n",
        "      },\n",
        "      \"isHighRisk\": True,\n",
        "      \"isActive\": False\n",
        "    },\n",
        "    {\n",
        "      \"name\": \"pii_eq_1\",\n",
        "      \"type\": \"PII\",\n",
        "      \"threshold\": {\n",
        "        \"eq\": 0.23\n",
        "      },\n",
        "      \"isHighRisk\": False,\n",
        "      \"isActive\": True\n",
        "    },\n",
        "    {\n",
        "      \"name\": \"pii_eq_2\",\n",
        "      \"type\": \"PII\",\n",
        "      \"threshold\": {\n",
        "        \"eq\": 0.23\n",
        "      },\n",
        "      \"isHighRisk\": False,\n",
        "      \"isActive\": False\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"toxicity_demo\",\n",
        "        \"type\": \"Toxicity\",\n",
        "        \"threshold\": {\n",
        "            \"eq\": 0.5\n",
        "        },\n",
        "        \"isHighRisk\": False,\n",
        "        \"isActive\": True\n",
        "        }  \n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gdm.add_guardrails(guardrails, guardrails_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "guardrails_config = {}\n",
        "# print(guardrails_config[\"abc\"])\n",
        "guardrails_config.get(\"isActive\",False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "payload = json.dumps({\n",
        "  \"isActive\": True,\n",
        "  \"guardrailFailConditions\": [\n",
        "    \"FAIL\",\n",
        "    \"TIMEOUT\"\n",
        "  ],\n",
        "  \"deploymentFailCondition\": \"ONE_FAIL\",\n",
        "  \"failAction\": {\n",
        "    \"action\": \"ALTERNATE_RESPONSE\",\n",
        "    \"args\": \"{\\\"alternateResponse\\\": \\\"This is the Alternate Response\\\"}\"\n",
        "  },\n",
        "  \"guardrails\": [\n",
        "    {\n",
        "      \"name\": \"pii_lte\",\n",
        "      \"type\": \"PII\",\n",
        "      \"threshold\": {\n",
        "        \"lte\": 0.23\n",
        "      },\n",
        "      \"isHighRisk\": True,\n",
        "      \"isActive\": True\n",
        "    }\n",
        "  ]\n",
        "})\n",
        "payload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "url = \"http://13.200.236.233/api/guardrail/deployment/11/configure\"\n",
        "\n",
        "payload = json.dumps({\n",
        "  \"isActive\": True,\n",
        "  \"guardrailFailConditions\": [\n",
        "    \"FAIL\",\n",
        "    \"TIMEOUT\"\n",
        "  ],\n",
        "  \"deploymentFailCondition\": \"ONE_FAIL\",\n",
        "  \"failAction\": {\n",
        "    \"action\": \"ALTERNATE_RESPONSE\",\n",
        "    \"args\": \"{\\\"alternateResponse\\\": \\\"This is the Alternate Response\\\"}\"\n",
        "  },\n",
        "  \"guardrails\": [\n",
        "    {\n",
        "      \"name\": \"pii_lte\",\n",
        "      \"type\": \"PII\",\n",
        "      \"threshold\": {\n",
        "        \"lte\": 0.23\n",
        "      },\n",
        "      \"isHighRisk\": True,\n",
        "      \"isActive\": True\n",
        "    }\n",
        "  ]\n",
        "})\n",
        "headers = {\n",
        "  'Authorization': 'Bearer eyJhbGciOiJIUzUxMiJ9.eyJ3cml0ZVByb2plY3RJZHMiOlsxLDIsM10sInN1YiI6ImFkbWluQHJhZ2EiLCJvcmdOYW1lIjoiUmFnYS1BSSIsIm9yZ0RvbWFpbiI6InJhZ2EiLCJyb2xlcyI6WyJST0xFX1NVUEVSIl0sInVzZXJGdWxsTmFtZSI6IlRlc3QgVXNlciIsInVzZXJOYW1lIjoiYWRtaW5AcmFnYSIsInVzZXJJZCI6MSwib3JnSWQiOjEsInJlYWRQcm9qZWN0SWRzIjpbMSwyLDNdLCJleHAiOjE3Mjk4NDI2OTYsImlhdCI6MTcyOTc1NjI5NiwianRpIjoiMSJ9.9AkfqPD4ldR7r-tWPeo9fNTS3BaWKGB9FYRPMpopXmxpS6GIQBjvusDnNnJyVhuHKQvuFNcvsilLGfqWNdjPng',\n",
        "  'Content-Type': 'application/json',\n",
        "  'X-Project-Id': '2'\n",
        "}\n",
        "\n",
        "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "payload = json.dumps({\n",
        "  \"isActive\": True,\n",
        "  \"guardrailFailConditions\": [\n",
        "    \"FAIL\",\n",
        "    \"TIMEOUT\"\n",
        "  ],\n",
        "  \"deploymentFailCondition\": \"ONE_FAIL\",\n",
        "  \"failAction\": {\n",
        "    \"action\": \"ALTERNATE_RESPONSE\",\n",
        "    \"args\": \"{\\\"alternateResponse\\\": \\\"This is the Alternate Response\\\"}\"\n",
        "  },\n",
        "  \"guardrails\": [\n",
        "    {\n",
        "      \"name\": \"pii_lte\",\n",
        "      \"type\": \"PII\",\n",
        "      \"threshold\": {\n",
        "        \"lte\": 0.23\n",
        "      },\n",
        "      \"isHighRisk\": True,\n",
        "      \"isActive\": True\n",
        "    }\n",
        "  ]\n",
        "})\n",
        "payload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'{\"isActive\": true, \"guardrailFailConditions\": [\"TIMEOUT\"], \"deploymentFailCondition\": \"ALL_FAIL\", \"failAction\": {\"action\": \"ALTERNATE_RESPONSE\", \"args\": \"{\\'alternateResponse\\': \\'This is alternate\\'}\"}, \"guardrails\": [{\"name\": \"pii_lte\", \"type\": \"PII\", \"isHighRisk\": true, \"isActive\": true, \"threshold\": {\"lte\": 0.23}}, {\"name\": \"pii_gte\", \"type\": \"PII\", \"isHighRisk\": true, \"isActive\": false, \"threshold\": {\"gte\": 0.23}}, {\"name\": \"pii_eq_1\", \"type\": \"PII\", \"isHighRisk\": false, \"isActive\": true, \"threshold\": {\"eq\": 0.23}}, {\"name\": \"pii_eq_2\", \"type\": \"PII\", \"isHighRisk\": false, \"isActive\": false, \"threshold\": {\"eq\": 0.23}}, {\"name\": \"toxicity_demo\", \"type\": \"Toxicity\", \"isHighRisk\": false, \"isActive\": true, \"threshold\": {\"eq\": 0.5}}]}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "original_list = [\n",
        "    {'PII': 'pii_gte'},\n",
        "    {'PII': 'pii_lte'},\n",
        "    {'PII': 'pii_eq_2'},\n",
        "    {'Toxicity': 'toxicity_demo'},\n",
        "    {'PII': 'pii_eq_1'}\n",
        "]\n",
        "\n",
        "# Extract values into new list\n",
        "new_list = [list(d.values())[0] for d in original_list]\n",
        "\n",
        "print(new_list)\n",
        "# ['pii_gte', 'pii_lte', 'pii_eq_2', 'toxicity_demo', 'pii_eq_1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list(original_list[0].values())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_condif = {\n",
        " \"model_name\": \"abc\",\n",
        " \"te,perature\": 0.6,\n",
        " \"max_token\": 10\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "url = \"http://13.200.11.66:4000/chat/completions\"\n",
        "\n",
        "payload = json.dumps(\n",
        "        {\n",
        "    \"model\": model_config.pop(\"model\"),\n",
        "    **model_config,\n",
        "\n",
        "    \"messages\": [\n",
        "        {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"you are analyst\"\n",
        "        },\n",
        "        {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"tell me the name of best indian player\"\n",
        "        }\n",
        "    ],\n",
        "    \"user_id\": 1\n",
        "    }\n",
        ")\n",
        "headers = {\n",
        "  'Content-Type': 'application/json',\n",
        "  'Content-Type': 'application/json'\n",
        "}\n",
        "\n",
        "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## synthetic_diwakaer raised issue try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "import os\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragaai_catalyst import SyntheticDataGeneration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "synthetic_data_generation = SyntheticDataGeneration()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_file = \"/Users/siddharthakosti/Downloads/langchain_callbacks/ai document_061023_2.pdf\"\n",
        "text = synthetic_data_generation.process_document(input_data=text_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res_shape = 0\n",
        "while res_shape != 1:\n",
        "    issue_res = synthetic_data_generation.generate_qna(text, question_type ='mcq',model_config={\"provider\":\"gemini\",\"model\":\"gemini/gemini-1.5-flash\"},n=10)\n",
        "    res_shape = issue_res.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "issue_res.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
