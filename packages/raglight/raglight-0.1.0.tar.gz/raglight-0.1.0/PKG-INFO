Metadata-Version: 2.1
Name: raglight
Version: 0.1.0
Summary: A lightweight and modular framework for Retrieval-Augmented Generation (RAG)
Home-page: https://github.com/Bessouat40/rag-example
Author: Your Name
Author-email: your.email@example.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.11
Description-Content-Type: text/markdown

# RAGLight

**RAGLight** is a lightweight and modular framework for implementing **Retrieval-Augmented Generation (RAG)**. It enhances the capabilities of Large Language Models (LLMs) by combining document retrieval with natural language inference.

Designed for simplicity and flexibility, RAGLight leverages **Ollama** for LLM interaction and vector embeddings for efficient document similarity searches, making it an ideal tool for building context-aware AI solutions. ‚ú®

---

## Features

- üåê **Embeddings Model**: Uses `all-MiniLM-L6-v2` for creating compact and efficient vector embeddings, ideal for document similarity searches.
- üßôüèΩ **LLM Integration**: Employs `llama3` for natural language inference, enabling human-like and context-aware responses.
- ‚öñÔ∏è **RAG Pipeline**: Seamlessly integrates document retrieval with natural language generation into a unified workflow.
- üñãÔ∏è **PDF Support**: Supports ingestion and indexing of PDF files for easy querying.

---

## Prerequisites

Before you get started, make sure you have the following:

- **Python**: Version >= 3.11
- **Ollama Client**: Ensure you have a properly configured Ollama client. You may need an API key or a local Ollama server instance.
- **Python Dependencies**: See the Installation section below.

---

## Installation

### 1. Clone the Repository

```bash
git clone https://github.com/Bessouat40/rag-example.git
cd rag-example
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Configure Environment Variables

```bash
mv .env.example .env
```

Then fill in the `.env` file with the necessary configuration:

```bash
# Example configuration
OLLAMA_CLIENT=<URL or key for the Ollama client>
PERSIST_DIRECTORY=<Path to store inference data>
PERSIST_DIRECTORY_INGESTION=<Path to store ingestion data>
MODEL_EMBEDDINGS=all-MiniLM-L6-v2
MODEL_NAME=llama3
SYSTEM_PROMPT_DIRECTORY=<Path to the system prompt file>
COLLECTION_NAME=<Collection name for inference>
COLLECTION_NAME_INGESTION=<Collection name for ingestion>
DATA_PATH=./data
```

---

## Document Ingestion

To ingest your files (currently only PDF files are supported), place them in the `data` folder or the path specified by the `DATA_PATH` variable in the `.env` file.

Run the following script to index the documents:

```bash
python ingestion_example.py
```

This script:

- ‚è≥ Loads the embeddings model specified in `.env`.
- üéÆ Uses the `VectorStore` (Chroma) to index the documents.
- üîê Creates a persistent index in the directory defined by `PERSIST_DIRECTORY_INGESTION`.

---

## Query the Model (RAG Pipeline)

To query the RAG pipeline, use the following script:

```bash
python rag_example.py
```

The pipeline:

- üîç Retrieves the most relevant documents using the vector model.
- ü§ñ Uses the `llama3` model to generate a response based on the retrieved context.

---

## TODO

- [ ] **Feature**: Add the possibility to use custom pipelines while ingesting data into the Vector Store.
- [ ] **Feature**: Add support for new Vector Stores (e.g., FAISS, Weaviate, Milvus).
- [ ] **Feature**: Integrate new LLM providers (e.g., VLLM, HuggingFace, GPT-Neo).

---
